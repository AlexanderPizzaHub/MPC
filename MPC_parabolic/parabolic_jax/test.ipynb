{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp \n",
    "from jax import grad, jit, vmap, hessian\n",
    "from jax import random\n",
    "import jax.config  as config \n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import jax \n",
    "from utils import gt\n",
    "import os \n",
    "import jaxopt\n",
    "from jax.flatten_util import ravel_pytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(\"jax_enable_x64\", True)\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './results/t1'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "layers = [3,20,20,1]\n",
    "max_iter = 2000\n",
    "val_int = 10\n",
    "bw = 100 \n",
    "iw = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = jax.nn.initializers.glorot_uniform()\n",
    "def random_layer_params(key,m,n,scale=1):\n",
    "    return scale * initializer(key, (n,m),dtype=jnp.float64), jnp.zeros((n,))\n",
    "\n",
    "def init_network_params(key,sizes):\n",
    "    return [random_layer_params(key,x,y) for x,y in zip(sizes[:-1],sizes[1:])]\n",
    "\n",
    "#params: [[[L1,L2],[L1,]],[[L2,L3],[L2,]],...,[[Ln-1,Ln],[Ln-1,]]]\n",
    "#        list -> tuple -> array\n",
    "#        layers -> weight, bias -> coefficients\n",
    "\n",
    "def NN(activation):\n",
    "    def model(params,x):\n",
    "        output = x \n",
    "        for w,b in params[:-1]:\n",
    "            #print(output.shape,jnp.dot(output,w.T).shape,b.shape)\n",
    "            linear = jnp.dot(output,w.T) + b\n",
    "            output = activation(linear)\n",
    "        w,b = params[-1]\n",
    "        #print(output.shape,jnp.dot(output,w.T).shape,b.shape)\n",
    "        output = jnp.reshape(jnp.dot(output,w.T) + b,())\n",
    "        return output\n",
    "    return model \n",
    "\n",
    "params = init_network_params(key,layers)\n",
    "u = NN(jnp.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace(func):\n",
    "\n",
    "    hess = hessian(func,0)\n",
    "\n",
    "    lap = lambda x,t: jnp.trace(hess(x,t))\n",
    "\n",
    "    return lap \n",
    "\n",
    "def parabolic(func):\n",
    "    lap = laplace(func)\n",
    "    time_diff = grad(func,1)\n",
    "    par = lambda x: time_diff(x[:-1],x[-1])+lap(x[:-1],x[-1])\n",
    "\n",
    "    return par \n",
    "\n",
    "def bdry(func):\n",
    "    b = lambda x: func(x)\n",
    "    return b \n",
    "\n",
    "def init(func):\n",
    "    i = lambda x:func(x) \n",
    "    return i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "loading data\n",
    "'''\n",
    "\n",
    "with open(\"dataset/2000pts\",'rb') as pfile:\n",
    "    data = pkl.load(pfile)\n",
    "\n",
    "d_c = data['domain']\n",
    "b_c = data['bdry']\n",
    "i_c = data['init']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: (1-2*jnp.pi**2*x[2])*jnp.sin(jnp.pi*x[0])*jnp.sin(jnp.pi*x[1])\n",
    "f_d = vmap(f)(d_c) \n",
    "\n",
    "b_d = jnp.zeros([len(b_c),1])\n",
    "i_d = jnp.zeros([len(i_c),1]) + 0.02*jax.random.normal(key,[len(i_c),1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parab = lambda param: parabolic(lambda x,t: u(param,jnp.hstack([x,t])))\n",
    "bdry_func = lambda param: bdry(lambda x: u(param,x))\n",
    "init_func = lambda param: init(lambda x: u(param,x))\n",
    "\n",
    "pred_d = lambda param,x : parab(param)(x)\n",
    "pred_b = lambda param,x : bdry_func(param)(x)\n",
    "pred_i = lambda param,x : init_func(param)(x)\n",
    "\n",
    "v_pred_d = jit(vmap(pred_d,(None,0)))\n",
    "v_pred_b = jit(vmap(pred_b,(None,0)))\n",
    "v_pred_i = jit(vmap(pred_i,(None,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit \n",
    "def loss(param):\n",
    "    l_d = jnp.mean((v_pred_d(param,d_c) - f_d)**2)\n",
    "    l_b = jnp.mean((v_pred_b(param,b_c) - b_d)**2)\n",
    "    l_i = jnp.mean((v_pred_i(param,i_c) - i_d)**2)\n",
    "\n",
    "    return l_d + bw * l_b + iw * l_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 | Error 0.5733184934599507\n",
      "Iter 10 | Error 0.5715436295550633\n",
      "Iter 20 | Error 0.26020823329845144\n",
      "Iter 30 | Error 0.17649193408349287\n",
      "Iter 40 | Error 0.19420168480772135\n",
      "Iter 50 | Error 0.13825586041393503\n",
      "Iter 60 | Error 0.08043049583386293\n",
      "Iter 70 | Error 0.07632401381894778\n",
      "Iter 80 | Error 0.06685910874356864\n",
      "Iter 90 | Error 0.06286070710762598\n",
      "Iter 100 | Error 0.05520329395818939\n",
      "Iter 110 | Error 0.04651274479855597\n",
      "Iter 120 | Error 0.04353244797828913\n",
      "Iter 130 | Error 0.050420364343653275\n",
      "Iter 140 | Error 0.04841416700522314\n",
      "Iter 150 | Error 0.05125786502559185\n",
      "Iter 160 | Error 0.04559126286478243\n",
      "Iter 170 | Error 0.048203376378351925\n",
      "Iter 180 | Error 0.04466500995281347\n",
      "Iter 190 | Error 0.046325493715804786\n",
      "Iter 200 | Error 0.042965565338742094\n",
      "Iter 210 | Error 0.038907981240841986\n",
      "Iter 220 | Error 0.03861442370026865\n",
      "Iter 230 | Error 0.0399321935099246\n",
      "Iter 240 | Error 0.038378958498781104\n",
      "Iter 250 | Error 0.038382454036806464\n",
      "Iter 260 | Error 0.03501270349788849\n",
      "Iter 270 | Error 0.03168676382487039\n",
      "Iter 280 | Error 0.03292229073174566\n",
      "Iter 290 | Error 0.03397976497603335\n",
      "Iter 300 | Error 0.03447936600091996\n",
      "Iter 310 | Error 0.03458480717569714\n",
      "Iter 320 | Error 0.034060577772153974\n",
      "Iter 330 | Error 0.034632308650889225\n",
      "Iter 340 | Error 0.03510664316307467\n",
      "Iter 350 | Error 0.03426240291262438\n",
      "Iter 360 | Error 0.03226424436781263\n",
      "Iter 370 | Error 0.03166142468074784\n",
      "Iter 380 | Error 0.031000236924046794\n",
      "Iter 390 | Error 0.029165853286357374\n",
      "Iter 400 | Error 0.028383401619406518\n",
      "Iter 410 | Error 0.02823870788515981\n",
      "Iter 420 | Error 0.028663523718490465\n",
      "Iter 430 | Error 0.026969669751008153\n",
      "Iter 440 | Error 0.02638340124018803\n",
      "Iter 450 | Error 0.02632384118713457\n",
      "Iter 460 | Error 0.025533878822925214\n",
      "Iter 470 | Error 0.026020368494687807\n",
      "Iter 480 | Error 0.02375455383357703\n",
      "Iter 490 | Error 0.023796395729496045\n",
      "Iter 500 | Error 0.023008856961433156\n",
      "Iter 510 | Error 0.022694640760813088\n",
      "Iter 520 | Error 0.022365956135902226\n",
      "Iter 530 | Error 0.0204735443670212\n",
      "Iter 540 | Error 0.02056391892970558\n",
      "Iter 550 | Error 0.020686697350473435\n",
      "Iter 560 | Error 0.0200985008009654\n",
      "Iter 570 | Error 0.02083266240382743\n",
      "Iter 580 | Error 0.01961261077418126\n",
      "Iter 590 | Error 0.01982911700811051\n",
      "Iter 600 | Error 0.020508320844163378\n",
      "Iter 610 | Error 0.020173542979671114\n",
      "Iter 620 | Error 0.020682732271351305\n",
      "Iter 630 | Error 0.020559229818472104\n",
      "Iter 640 | Error 0.02003506804568979\n",
      "Iter 650 | Error 0.01979555654421646\n",
      "Iter 660 | Error 0.019826960268401708\n",
      "Iter 670 | Error 0.02070802995012296\n",
      "Iter 680 | Error 0.01904693076930236\n",
      "Iter 690 | Error 0.01883073670452583\n",
      "Iter 700 | Error 0.0181115707820611\n",
      "Iter 710 | Error 0.01777945214310939\n",
      "Iter 720 | Error 0.017283876408795102\n",
      "Iter 730 | Error 0.01707308340091597\n",
      "Iter 740 | Error 0.01695469656190695\n",
      "Iter 750 | Error 0.016999014960413283\n",
      "Iter 760 | Error 0.016567033304318114\n",
      "Iter 770 | Error 0.015121192957884638\n",
      "Iter 780 | Error 0.014785154156920575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 790 | Error 0.015287799549072835\n"
     ]
    }
   ],
   "source": [
    "y = lambda x: jnp.sin(jnp.pi*x[0])*jnp.sin(jnp.pi*x[1])*x[2]\n",
    "\n",
    "res = lambda param,x : (u(param,x)-y(x))**2 \n",
    "v_res = vmap(res,(None,0))\n",
    "\n",
    "\n",
    "flat_param, unravel = ravel_pytree(params)\n",
    "flat_loss = lambda flat_param: loss(unravel(flat_param))\n",
    "\n",
    "LBFGS = jaxopt.LBFGS(fun=flat_loss,value_and_grad=False,linesearch='hager-zhang')\n",
    "state = LBFGS.init_state(flat_param) \n",
    "\n",
    "for iter in jnp.arange(0,max_iter):\n",
    "    flat_param,state = LBFGS.update(flat_param,state)\n",
    "    if iter % val_int == 0:\n",
    "        params = unravel(flat_param)\n",
    "        error = jnp.sqrt(jnp.mean(v_res(params,d_c)))\n",
    "        print(\"Iter {} | Error {}\".format(iter,error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
